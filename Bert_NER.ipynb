{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "IiTvhlk63tlp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb5b8ba6-9669-483d-fda2-8c3eb164ea94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import transformers\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn import model_selection\n",
        "\n",
        "from tqdm import tqdm\n",
        "from transformers import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup"
      ],
      "metadata": {
        "id": "X-P_j5py3i4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class config:\n",
        "    MAX_LEN = 128\n",
        "    TRAIN_BATCH_SIZE = 64\n",
        "    VALID_BATCH_SIZE = 64\n",
        "    EPOCHS = 5\n",
        "    BASE_MODEL_PATH = \"bert-base-uncased\"\n",
        "    MODEL_PATH = \"model.bin\"\n",
        "    TRAINING_FILE = \"/content/drive/MyDrive/NLP/ner_dataset.csv\"\n",
        "    TOKENIZER = transformers.BertTokenizer.from_pretrained(\n",
        "        BASE_MODEL_PATH,\n",
        "        do_lower_case=True\n",
        "    )"
      ],
      "metadata": {
        "id": "fi4c3UbQ34AA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_data(data_path):\n",
        "    df = pd.read_csv(data_path, encoding=\"latin-1\")\n",
        "    df.loc[:, \"Sentence #\"] = df[\"Sentence #\"].fillna(method=\"ffill\")\n",
        "\n",
        "    enc_pos = preprocessing.LabelEncoder()\n",
        "    enc_tag = preprocessing.LabelEncoder()\n",
        "\n",
        "    df.loc[:, \"POS\"] = enc_pos.fit_transform(df[\"POS\"]) # fit_transform : a function that transforms the str labels into int labels\n",
        "    df.loc[:, \"Tag\"] = enc_tag.fit_transform(df[\"Tag\"]) # and return the int \n",
        "\n",
        "    sentences = df.groupby(\"Sentence #\")[\"Word\"].apply(list).values\n",
        "    pos = df.groupby(\"Sentence #\")[\"POS\"].apply(list).values\n",
        "    tag = df.groupby(\"Sentence #\")[\"Tag\"].apply(list).values\n",
        "    return sentences, pos, tag, enc_pos, enc_tag"
      ],
      "metadata": {
        "id": "oAI5HNo74Reg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EntityDataset:\n",
        "    def __init__(self, texts, pos, tags):\n",
        "        self.texts = texts\n",
        "        self.pos = pos\n",
        "        self.tags = tags\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "    \n",
        "    def __getitem__(self, item):\n",
        "        text = self.texts[item]\n",
        "        pos = self.pos[item]\n",
        "        tags = self.tags[item]\n",
        "\n",
        "        ids = []\n",
        "        target_pos = []\n",
        "        target_tag =[]\n",
        "\n",
        "        for i, s in enumerate(text): \n",
        "            inputs = config.TOKENIZER.encode(\n",
        "                s,\n",
        "                add_special_tokens=False\n",
        "            )\n",
        "            # abhishek: ab ##hi ##sh ##ek\n",
        "            input_len = len(inputs)\n",
        "            ids.extend(inputs) # add all the sub-words\n",
        "            target_pos.extend([pos[i]] * input_len) # all the sub-words from one word are labeled with the same pos\n",
        "            target_tag.extend([tags[i]] * input_len) # all the sub-words from one word are labeled with the same tag\n",
        "\n",
        "        ids = ids[:config.MAX_LEN - 2] # leave space for [CLS][SEP]\n",
        "        target_pos = target_pos[:config.MAX_LEN - 2] \n",
        "        target_tag = target_tag[:config.MAX_LEN - 2]\n",
        "\n",
        "        ids = [101] + ids + [102] # 101 -> CLS  102 -> SEP\n",
        "        target_pos = [0] + target_pos + [0]\n",
        "        target_tag = [0] + target_tag + [0]\n",
        "\n",
        "        mask = [1] * len(ids)\n",
        "        token_type_ids = [0] * len(ids)\n",
        "\n",
        "        padding_len = config.MAX_LEN - len(ids)\n",
        "\n",
        "        ids = ids + ([0] * padding_len) # make all the input the same length\n",
        "        mask = mask + ([0] * padding_len) # 0->acceptable 1->omitted\n",
        "        token_type_ids = token_type_ids + ([0] * padding_len)\n",
        "        target_pos = target_pos + ([0] * padding_len)\n",
        "        target_tag = target_tag + ([0] * padding_len)\n",
        "\n",
        "        return {\n",
        "            \"ids\": torch.tensor(ids, dtype=torch.long),\n",
        "            \"mask\": torch.tensor(mask, dtype=torch.long),\n",
        "            \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n",
        "            \"target_pos\": torch.tensor(target_pos, dtype=torch.long),\n",
        "            \"target_tag\": torch.tensor(target_tag, dtype=torch.long),\n",
        "        }"
      ],
      "metadata": {
        "id": "cTl7IJUY4Rnv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_fn(data_loader, model, optimizer, device, scheduler):\n",
        "    model.train()\n",
        "    final_loss = 0\n",
        "    for data in tqdm(data_loader, total=len(data_loader)):\n",
        "        for k, v in data.items():\n",
        "            data[k] = v.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        _, _, loss = model(**data)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        final_loss += loss.item()\n",
        "    return final_loss / len(data_loader)\n",
        "\n",
        "\n",
        "def eval_fn(data_loader, model, device):\n",
        "    model.eval()\n",
        "    final_loss = 0\n",
        "    for data in tqdm(data_loader, total=len(data_loader)):\n",
        "        for k, v in data.items():\n",
        "            data[k] = v.to(device)\n",
        "        _, _, loss = model(**data)\n",
        "        final_loss += loss.item()\n",
        "    return final_loss / len(data_loader)"
      ],
      "metadata": {
        "id": "4ykgrGsN4Rlk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fn(output, target, mask, num_labels):\n",
        "    lfn = nn.CrossEntropyLoss()\n",
        "    active_loss = mask.view(-1) == 1\n",
        "    active_logits = output.view(-1, num_labels)\n",
        "    active_labels = torch.where(\n",
        "        active_loss,\n",
        "        target.view(-1),\n",
        "        torch.tensor(lfn.ignore_index).type_as(target)\n",
        "    )\n",
        "    loss = lfn(active_logits, active_labels)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "FJsB5F2W4RjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EntityModel(nn.Module):\n",
        "    def __init__(self, num_tag, num_pos):\n",
        "        super(EntityModel, self).__init__()\n",
        "        self.num_tag = num_tag\n",
        "        self.num_pos = num_pos\n",
        "        self.bert = transformers.BertModel.from_pretrained(\n",
        "            config.BASE_MODEL_PATH\n",
        "        )\n",
        "        self.bert_drop_1 = nn.Dropout(0.3)\n",
        "        self.bert_drop_2 = nn.Dropout(0.3)\n",
        "        self.out_tag = nn.Linear(768, self.num_tag)\n",
        "        self.out_pos = nn.Linear(768, self.num_pos)\n",
        "    \n",
        "    def forward(\n",
        "        self, \n",
        "        ids, \n",
        "        mask, \n",
        "        token_type_ids, \n",
        "        target_pos, \n",
        "        target_tag\n",
        "    ):\n",
        "        o1, _ = self.bert(\n",
        "            ids, \n",
        "            attention_mask=mask, \n",
        "            token_type_ids=token_type_ids,\n",
        "            return_dict=False\n",
        "        )\n",
        "\n",
        "        bo_tag = self.bert_drop_1(o1)\n",
        "        bo_pos = self.bert_drop_2(o1)\n",
        "\n",
        "        tag = self.out_tag(bo_tag)\n",
        "        pos = self.out_pos(bo_pos)\n",
        "\n",
        "        loss_tag = loss_fn(tag, target_tag, mask, self.num_tag)\n",
        "        loss_pos = loss_fn(pos, target_pos, mask, self.num_pos)\n",
        "\n",
        "        loss = 0.5*loss_tag + 0.5*loss_pos\n",
        "\n",
        "        return tag, pos, loss"
      ],
      "metadata": {
        "id": "PFjDR4v74Rg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences, pos, tag, enc_pos, enc_tag = process_data(config.TRAINING_FILE)\n",
        "\n",
        "meta_data = {\n",
        "    \"enc_pos\": enc_pos,\n",
        "    \"enc_tag\": enc_tag\n",
        "}\n",
        "\n",
        "joblib.dump(meta_data, \"meta.bin\")\n",
        "\n",
        "num_pos = len(list(enc_pos.classes_))\n",
        "num_tag = len(list(enc_tag.classes_))\n",
        "\n",
        "(\n",
        "    train_sentences,\n",
        "    test_sentences,\n",
        "    train_pos,\n",
        "    test_pos,\n",
        "    train_tag,\n",
        "    test_tag\n",
        ") = model_selection.train_test_split(\n",
        "    sentences, \n",
        "    pos, \n",
        "    tag, \n",
        "    random_state=42, \n",
        "    test_size=0.1\n",
        ")\n",
        "\n",
        "train_dataset = EntityDataset(\n",
        "    texts=train_sentences, pos=train_pos, tags=train_tag\n",
        ")\n",
        "\n",
        "train_data_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=config.TRAIN_BATCH_SIZE, num_workers=4\n",
        ")\n",
        "\n",
        "valid_dataset = EntityDataset(\n",
        "    texts=test_sentences, pos=test_pos, tags=test_tag\n",
        ")\n",
        "\n",
        "valid_data_loader = torch.utils.data.DataLoader(\n",
        "    valid_dataset, batch_size=config.VALID_BATCH_SIZE, num_workers=1\n",
        ")\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "model = EntityModel(num_tag=num_tag, num_pos=num_pos)\n",
        "model.to(device)\n",
        "\n",
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
        "optimizer_parameters = [\n",
        "    {\n",
        "        \"params\": [\n",
        "            p for n, p in param_optimizer if not any(\n",
        "                nd in n for nd in no_decay\n",
        "            )\n",
        "        ],\n",
        "        \"weight_decay\": 0.001,\n",
        "    },\n",
        "    {\n",
        "        \"params\": [\n",
        "            p for n, p in param_optimizer if any(\n",
        "                nd in n for nd in no_decay\n",
        "            )\n",
        "        ],\n",
        "        \"weight_decay\": 0.0,\n",
        "    },\n",
        "]\n",
        "\n",
        "num_train_steps = int(\n",
        "    len(train_sentences) / config.TRAIN_BATCH_SIZE * config.EPOCHS\n",
        ")\n",
        "optimizer = AdamW(optimizer_parameters, lr=3e-5)\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer, \n",
        "    num_warmup_steps=450, \n",
        "    num_training_steps=num_train_steps\n",
        ")"
      ],
      "metadata": {
        "id": "kBoPBreO4RcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_loss = np.inf\n",
        "for epoch in range(config.EPOCHS):\n",
        "    train_loss = train_fn(\n",
        "        train_data_loader, \n",
        "        model, \n",
        "        optimizer, \n",
        "        device, \n",
        "        scheduler\n",
        "    )\n",
        "    test_loss = eval_fn(\n",
        "        valid_data_loader,\n",
        "        model,\n",
        "        device\n",
        "    )\n",
        "    print(f\"Train Loss = {train_loss} Valid Loss = {test_loss}\")\n",
        "    if test_loss < best_loss:\n",
        "        torch.save(model.state_dict(), config.MODEL_PATH)\n",
        "        best_loss = test_loss"
      ],
      "metadata": {
        "id": "6_bIhWiaBrCX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e54d7aa-50ba-40c3-8b02-a78ccdd63701"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 675/675 [13:19<00:00,  1.19s/it]\n",
            "100%|██████████| 75/75 [00:33<00:00,  2.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss = 0.7659795353699613 Valid Loss = 0.132436350385348\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 675/675 [13:39<00:00,  1.21s/it]\n",
            "100%|██████████| 75/75 [00:33<00:00,  2.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss = 0.12268776214785046 Valid Loss = 0.1081779341896375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 58%|█████▊    | 390/675 [07:54<05:46,  1.21s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "meta_data = joblib.load(\"meta.bin\")\n",
        "enc_pos = meta_data[\"enc_pos\"]\n",
        "enc_tag = meta_data[\"enc_tag\"]\n",
        "\n",
        "num_pos = len(list(enc_pos.classes_))\n",
        "num_tag = len(list(enc_tag.classes_))\n",
        "\n",
        "sentence = \"\"\"\n",
        "abhishek is going to india\n",
        "\"\"\"\n",
        "tokenized_sentence = config.TOKENIZER.encode(sentence)\n",
        "\n",
        "sentence = sentence.split()\n",
        "print(sentence)\n",
        "print(tokenized_sentence)\n",
        "\n",
        "test_dataset = EntityDataset(\n",
        "    texts=[sentence], \n",
        "    pos=[[0] * len(sentence)], \n",
        "    tags=[[0] * len(sentence)]\n",
        ")\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "model = EntityModel(num_tag=num_tag, num_pos=num_pos)\n",
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/NLP/model.bin\"))\n",
        "model.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    data = test_dataset[0]\n",
        "    for k, v in data.items():\n",
        "        data[k] = v.to(device).unsqueeze(0)\n",
        "    tag, pos, _ = model(**data)\n",
        "\n",
        "    print(\n",
        "        enc_tag.inverse_transform(\n",
        "            tag.argmax(2).cpu().numpy().reshape(-1)\n",
        "        )[:len(tokenized_sentence)]\n",
        "    )\n",
        "    print(\n",
        "        enc_pos.inverse_transform(\n",
        "            pos.argmax(2).cpu().numpy().reshape(-1)\n",
        "        )[:len(tokenized_sentence)]\n",
        "    )"
      ],
      "metadata": {
        "id": "frCdEp3PPGeT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s = config.TOKENIZER.encode(\"abhishek is going to india\")\n",
        "config.TOKENIZER.convert_ids_to_tokens(s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTHHGkJmQ76s",
        "outputId": "32e84851-1e13-4aee-e0e1-c747b4be81b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS]', 'ab', '##his', '##he', '##k', 'is', 'going', 'to', 'india', '[SEP]']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv /content/model.bin /content/drive/MyDrive/NLP/"
      ],
      "metadata": {
        "id": "S7dnsRYJdTiQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}