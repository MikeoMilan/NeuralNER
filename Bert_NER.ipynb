{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "IiTvhlk63tlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import transformers\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn import model_selection\n",
        "\n",
        "from tqdm import tqdm\n",
        "from transformers import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup"
      ],
      "metadata": {
        "id": "X-P_j5py3i4S"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class config:\n",
        "    MAX_LEN = 128\n",
        "    TRAIN_BATCH_SIZE = 64\n",
        "    VALID_BATCH_SIZE = 64\n",
        "    EPOCHS = 5\n",
        "    BASE_MODEL_PATH = \"bert-base-uncased\"\n",
        "    MODEL_PATH = \"model.bin\"\n",
        "    TRAINING_FILE = \"/content/drive/MyDrive/NLP/ner_dataset.csv\"\n",
        "    TOKENIZER = transformers.BertTokenizer.from_pretrained(\n",
        "        BASE_MODEL_PATH,\n",
        "        do_lower_case=True\n",
        "    )"
      ],
      "metadata": {
        "id": "fi4c3UbQ34AA"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_data(data_path):\n",
        "    df = pd.read_csv(data_path, encoding=\"latin-1\")\n",
        "    df.loc[:, \"Sentence #\"] = df[\"Sentence #\"].fillna(method=\"ffill\")\n",
        "\n",
        "    enc_pos = preprocessing.LabelEncoder()\n",
        "    enc_tag = preprocessing.LabelEncoder()\n",
        "\n",
        "    df.loc[:, \"POS\"] = enc_pos.fit_transform(df[\"POS\"]) # fit_transform : a function that transforms the str labels into int labels\n",
        "    df.loc[:, \"Tag\"] = enc_tag.fit_transform(df[\"Tag\"]) # and return the int \n",
        "\n",
        "    sentences = df.groupby(\"Sentence #\")[\"Word\"].apply(list).values\n",
        "    pos = df.groupby(\"Sentence #\")[\"POS\"].apply(list).values\n",
        "    tag = df.groupby(\"Sentence #\")[\"Tag\"].apply(list).values\n",
        "    return sentences, pos, tag, enc_pos, enc_tag"
      ],
      "metadata": {
        "id": "oAI5HNo74Reg"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EntityDataset:\n",
        "    def __init__(self, texts, pos, tags):\n",
        "        self.texts = texts\n",
        "        self.pos = pos\n",
        "        self.tags = tags\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "    \n",
        "    def __getitem__(self, item):\n",
        "        text = self.texts[item]\n",
        "        pos = self.pos[item]\n",
        "        tags = self.tags[item]\n",
        "\n",
        "        ids = []\n",
        "        target_pos = []\n",
        "        target_tag =[]\n",
        "\n",
        "        for i, s in enumerate(text): \n",
        "            inputs = config.TOKENIZER.encode(\n",
        "                s,\n",
        "                add_special_tokens=False\n",
        "            )\n",
        "            # abhishek: ab ##hi ##sh ##ek\n",
        "            input_len = len(inputs)\n",
        "            ids.extend(inputs) # add all the sub-words\n",
        "            target_pos.extend([pos[i]] * input_len) # all the sub-words from one word are labeled with the same pos\n",
        "            target_tag.extend([tags[i]] * input_len) # all the sub-words from one word are labeled with the same tag\n",
        "\n",
        "        ids = ids[:config.MAX_LEN - 2] # leave space for [CLS][SEP]\n",
        "        target_pos = target_pos[:config.MAX_LEN - 2] \n",
        "        target_tag = target_tag[:config.MAX_LEN - 2]\n",
        "\n",
        "        ids = [101] + ids + [102] # 101 -> CLS  102 -> SEP\n",
        "        target_pos = [0] + target_pos + [0]\n",
        "        target_tag = [0] + target_tag + [0]\n",
        "\n",
        "        mask = [1] * len(ids)\n",
        "        token_type_ids = [0] * len(ids)\n",
        "\n",
        "        padding_len = config.MAX_LEN - len(ids)\n",
        "\n",
        "        ids = ids + ([0] * padding_len) # make all the input the same length\n",
        "        mask = mask + ([0] * padding_len) # 0->acceptable 1->omitted\n",
        "        token_type_ids = token_type_ids + ([0] * padding_len)\n",
        "        target_pos = target_pos + ([0] * padding_len)\n",
        "        target_tag = target_tag + ([0] * padding_len)\n",
        "\n",
        "        return {\n",
        "            \"ids\": torch.tensor(ids, dtype=torch.long),\n",
        "            \"mask\": torch.tensor(mask, dtype=torch.long),\n",
        "            \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n",
        "            \"target_pos\": torch.tensor(target_pos, dtype=torch.long),\n",
        "            \"target_tag\": torch.tensor(target_tag, dtype=torch.long),\n",
        "        }"
      ],
      "metadata": {
        "id": "cTl7IJUY4Rnv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_fn(data_loader, model, optimizer, device, scheduler):\n",
        "    model.train()\n",
        "    final_loss = 0\n",
        "    for data in tqdm(data_loader, total=len(data_loader)):\n",
        "        for k, v in data.items():\n",
        "            data[k] = v.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        _, _, loss = model(**data)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        final_loss += loss.item()\n",
        "    return final_loss / len(data_loader)\n",
        "\n",
        "\n",
        "def eval_fn(data_loader, model, device):\n",
        "    model.eval()\n",
        "    final_loss = 0\n",
        "    for data in tqdm(data_loader, total=len(data_loader)):\n",
        "        for k, v in data.items():\n",
        "            data[k] = v.to(device)\n",
        "        _, _, loss = model(**data)\n",
        "        final_loss += loss.item()\n",
        "    return final_loss / len(data_loader)"
      ],
      "metadata": {
        "id": "4ykgrGsN4Rlk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fn(output, target, mask, num_labels):\n",
        "    lfn = nn.CrossEntropyLoss()\n",
        "    active_loss = mask.view(-1) == 1\n",
        "    active_logits = output.view(-1, num_labels)\n",
        "    active_labels = torch.where(\n",
        "        active_loss,\n",
        "        target.view(-1),\n",
        "        torch.tensor(lfn.ignore_index).type_as(target)\n",
        "    )\n",
        "    loss = lfn(active_logits, active_labels)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "FJsB5F2W4RjL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EntityModel(nn.Module):\n",
        "    def __init__(self, num_tag, num_pos):\n",
        "        super(EntityModel, self).__init__()\n",
        "        self.num_tag = num_tag\n",
        "        self.num_pos = num_pos\n",
        "        self.bert = transformers.BertModel.from_pretrained(\n",
        "            config.BASE_MODEL_PATH\n",
        "        )\n",
        "        self.bert_drop_1 = nn.Dropout(0.3)\n",
        "        self.bert_drop_2 = nn.Dropout(0.3)\n",
        "        self.out_tag = nn.Linear(768, self.num_tag)\n",
        "        self.out_pos = nn.Linear(768, self.num_pos)\n",
        "    \n",
        "    def forward(\n",
        "        self, \n",
        "        ids, \n",
        "        mask, \n",
        "        token_type_ids, \n",
        "        target_pos, \n",
        "        target_tag\n",
        "    ):\n",
        "        o1, _ = self.bert(\n",
        "            ids, \n",
        "            attention_mask=mask, \n",
        "            token_type_ids=token_type_ids,\n",
        "            return_dict=False\n",
        "        )\n",
        "\n",
        "        bo_tag = self.bert_drop_1(o1)\n",
        "        bo_pos = self.bert_drop_2(o1)\n",
        "\n",
        "        tag = self.out_tag(bo_tag)\n",
        "        pos = self.out_pos(bo_pos)\n",
        "\n",
        "        loss_tag = loss_fn(tag, target_tag, mask, self.num_tag)\n",
        "        loss_pos = loss_fn(pos, target_pos, mask, self.num_pos)\n",
        "\n",
        "        loss = 0.5*loss_tag + 0.5*loss_pos\n",
        "\n",
        "        return tag, pos, loss"
      ],
      "metadata": {
        "id": "PFjDR4v74Rg5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences, pos, tag, enc_pos, enc_tag = process_data(config.TRAINING_FILE)\n",
        "\n",
        "meta_data = {\n",
        "    \"enc_pos\": enc_pos,\n",
        "    \"enc_tag\": enc_tag\n",
        "}\n",
        "\n",
        "joblib.dump(meta_data, \"meta.bin\")\n",
        "\n",
        "num_pos = len(list(enc_pos.classes_))\n",
        "num_tag = len(list(enc_tag.classes_))\n",
        "\n",
        "(\n",
        "    train_sentences,\n",
        "    test_sentences,\n",
        "    train_pos,\n",
        "    test_pos,\n",
        "    train_tag,\n",
        "    test_tag\n",
        ") = model_selection.train_test_split(\n",
        "    sentences, \n",
        "    pos, \n",
        "    tag, \n",
        "    random_state=42, \n",
        "    test_size=0.1\n",
        ")\n",
        "\n",
        "train_dataset = EntityDataset(\n",
        "    texts=train_sentences, pos=train_pos, tags=train_tag\n",
        ")\n",
        "\n",
        "train_data_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=config.TRAIN_BATCH_SIZE, num_workers=4\n",
        ")\n",
        "\n",
        "valid_dataset = EntityDataset(\n",
        "    texts=test_sentences, pos=test_pos, tags=test_tag\n",
        ")\n",
        "\n",
        "valid_data_loader = torch.utils.data.DataLoader(\n",
        "    valid_dataset, batch_size=config.VALID_BATCH_SIZE, num_workers=1\n",
        ")\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "model = EntityModel(num_tag=num_tag, num_pos=num_pos)\n",
        "model.to(device)\n",
        "\n",
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
        "optimizer_parameters = [\n",
        "    {\n",
        "        \"params\": [\n",
        "            p for n, p in param_optimizer if not any(\n",
        "                nd in n for nd in no_decay\n",
        "            )\n",
        "        ],\n",
        "        \"weight_decay\": 0.001,\n",
        "    },\n",
        "    {\n",
        "        \"params\": [\n",
        "            p for n, p in param_optimizer if any(\n",
        "                nd in n for nd in no_decay\n",
        "            )\n",
        "        ],\n",
        "        \"weight_decay\": 0.0,\n",
        "    },\n",
        "]\n",
        "\n",
        "num_train_steps = int(\n",
        "    len(train_sentences) / config.TRAIN_BATCH_SIZE * config.EPOCHS\n",
        ")\n",
        "optimizer = AdamW(optimizer_parameters, lr=3e-5)\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer, \n",
        "    num_warmup_steps=450, \n",
        "    num_training_steps=num_train_steps\n",
        ")"
      ],
      "metadata": {
        "id": "kBoPBreO4RcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_loss = np.inf\n",
        "for epoch in range(config.EPOCHS):\n",
        "    train_loss = train_fn(\n",
        "        train_data_loader, \n",
        "        model, \n",
        "        optimizer, \n",
        "        device, \n",
        "        scheduler\n",
        "    )\n",
        "    test_loss = eval_fn(\n",
        "        valid_data_loader,\n",
        "        model,\n",
        "        device\n",
        "    )\n",
        "    print(f\"Train Loss = {train_loss} Valid Loss = {test_loss}\")\n",
        "    if test_loss < best_loss:\n",
        "        torch.save(model.state_dict(), config.MODEL_PATH)\n",
        "        best_loss = test_loss"
      ],
      "metadata": {
        "id": "6_bIhWiaBrCX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e54d7aa-50ba-40c3-8b02-a78ccdd63701"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 675/675 [13:19<00:00,  1.19s/it]\n",
            "100%|██████████| 75/75 [00:33<00:00,  2.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss = 0.7659795353699613 Valid Loss = 0.132436350385348\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 675/675 [13:39<00:00,  1.21s/it]\n",
            "100%|██████████| 75/75 [00:33<00:00,  2.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss = 0.12268776214785046 Valid Loss = 0.1081779341896375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 58%|█████▊    | 390/675 [07:54<05:46,  1.21s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "umt53aTFm6wr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences, pos, tag, enc_pos, enc_tag = process_data(config.TRAINING_FILE)\n",
        "\n",
        "meta_data = {\n",
        "    \"enc_pos\": enc_pos,\n",
        "    \"enc_tag\": enc_tag\n",
        "}\n",
        "\n",
        "joblib.dump(meta_data, \"meta.bin\")\n",
        "\n",
        "num_pos = len(list(enc_pos.classes_))\n",
        "num_tag = len(list(enc_tag.classes_))"
      ],
      "metadata": {
        "id": "4uJMOXO8nhBs"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\")\n",
        "model = EntityModel(num_tag=num_tag, num_pos=num_pos)\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/NLP/Bert.bin'))\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "jtiKtD95m6uR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentences = np.load(\"/content/test_sentences.npy\", allow_pickle=True)\n",
        "test_tag = np.load(\"/content/test_tag.npy\", allow_pickle=True)\n",
        "\n",
        "test_dataset = EntityDataset(texts=test_sentences, pos=test_tag, tags=test_tag)\n",
        "\n",
        "test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64)"
      ],
      "metadata": {
        "id": "eOtHLF-Em6r4"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "y_labels = []\n",
        "y_preds = []\n",
        "\n",
        "for data in tqdm(test_data_loader, total=len(test_data_loader)):\n",
        "    for k, v in data.items():\n",
        "        data[k] = v.to(device)\n",
        "    \n",
        "    for i in range(len(data[\"mask\"])):\n",
        "        mask = data[\"mask\"][i].cpu().numpy()\n",
        "        target = data[\"target_tag\"][i].cpu().numpy() # 1,128\n",
        "        temp = []\n",
        "        for j in range(len(mask)):  \n",
        "          if mask[j] == 1:\n",
        "            temp.append(target[j])\n",
        "        y_preds.append(temp)\n",
        "        \n",
        "    \n",
        "    y_pred,_,_ = model(**data)\n",
        "    y_pred = y_pred.detach().cpu().numpy()\n",
        "    y_pred = np.argmax(y_pred, axis=-1) # 64,128\n",
        "    for i in range(len(data[\"mask\"])):\n",
        "        mask = data[\"mask\"][i].cpu().numpy()\n",
        "        temp = []\n",
        "        for j in range(len(mask)):  \n",
        "          if mask[j] == 1:\n",
        "            temp.append(y_pred[i][j])\n",
        "        y_labels.append(temp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1r208_kvm6pV",
        "outputId": "a41f132b-4584-4f5d-af95-6d6dd50de8d5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 38/38 [00:21<00:00,  1.75it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_true, y_pred = [], []\n",
        "for tags in y_labels:\n",
        "  for tag in tags:\n",
        "    y_true.append(tag)\n",
        "\n",
        "for tags in y_preds:\n",
        "  for tag in tags:\n",
        "    y_pred.append(tag)\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "target_names = list(enc_tag.classes_)\n",
        "print(classification_report(y_true, y_pred, target_names=target_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6Yx1BTHthZo",
        "outputId": "025c0c22-0cae-48fa-89c5-277fb22e6122"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-art       0.99      1.00      1.00      4801\n",
            "       B-eve       0.07      0.20      0.10         5\n",
            "       B-geo       0.90      0.85      0.88      3168\n",
            "       B-gpe       0.92      0.94      0.93       849\n",
            "       B-nat       0.00      0.00      0.00         0\n",
            "       B-org       0.70      0.79      0.74      1535\n",
            "       B-per       0.86      0.84      0.85      1375\n",
            "       B-tim       0.87      0.91      0.89      1094\n",
            "       I-art       0.00      0.00      0.00         0\n",
            "       I-eve       0.00      0.00      0.00         0\n",
            "       I-geo       0.73      0.78      0.75       432\n",
            "       I-gpe       0.00      0.00      0.00         0\n",
            "       I-nat       0.00      0.00      0.00         0\n",
            "       I-org       0.69      0.72      0.71      1054\n",
            "       I-per       0.92      0.87      0.89      1771\n",
            "       I-tim       0.82      0.82      0.82       336\n",
            "           O       0.99      0.99      0.99     47676\n",
            "\n",
            "    accuracy                           0.96     64096\n",
            "   macro avg       0.56      0.57      0.56     64096\n",
            "weighted avg       0.96      0.96      0.96     64096\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    }
  ]
}