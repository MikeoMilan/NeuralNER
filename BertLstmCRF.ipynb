{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install pytorch-crf"
      ],
      "metadata": {
        "id": "wZQh9GRPWLsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import transformers\n",
        "from torchcrf import CRF\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn import model_selection\n",
        "\n",
        "from tqdm import tqdm\n",
        "from transformers import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup"
      ],
      "metadata": {
        "id": "ddNGR-U4ZWzx"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class config:\n",
        "    MAX_LEN = 128\n",
        "    TRAIN_BATCH_SIZE = 64\n",
        "    VALID_BATCH_SIZE = 64\n",
        "    EPOCHS = 10\n",
        "    BASE_MODEL_PATH = \"bert-base-uncased\"\n",
        "    MODEL_PATH = \"model.bin\"\n",
        "    TRAINING_FILE = \"/content/drive/MyDrive/NLP/ner_dataset.csv\"\n",
        "    TOKENIZER = transformers.BertTokenizer.from_pretrained(\n",
        "        BASE_MODEL_PATH,\n",
        "        do_lower_case=True\n",
        "    )\n",
        "\n",
        "def process_data(data_path):\n",
        "    df = pd.read_csv(data_path, encoding=\"latin-1\")\n",
        "    df.loc[:, \"Sentence #\"] = df[\"Sentence #\"].fillna(method=\"ffill\")\n",
        "\n",
        "    enc_tag = preprocessing.LabelEncoder()\n",
        "    \n",
        "    # fit_transform : a function that transforms the str labels into int labels\n",
        "    df.loc[:, \"Tag\"] = enc_tag.fit_transform(df[\"Tag\"]) # and return the int \n",
        "\n",
        "    sentences = df.groupby(\"Sentence #\")[\"Word\"].apply(list).values\n",
        "    tag = df.groupby(\"Sentence #\")[\"Tag\"].apply(list).values\n",
        "    return sentences, tag, enc_tag\n",
        "\n",
        "class EntityDataset:\n",
        "    def __init__(self, texts, tags):\n",
        "        self.texts = texts\n",
        "        self.tags = tags\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "    \n",
        "    def __getitem__(self, item):\n",
        "        text = self.texts[item]\n",
        "        tags = self.tags[item]\n",
        "\n",
        "        ids = []\n",
        "        target_tag =[]\n",
        "\n",
        "        for i, s in enumerate(text): \n",
        "            inputs = config.TOKENIZER.encode(\n",
        "                s,\n",
        "                add_special_tokens=False\n",
        "            )\n",
        "            # abhishek: ab ##hi ##sh ##ek\n",
        "            input_len = len(inputs)\n",
        "            ids.extend(inputs) # add all the sub-words\n",
        "            target_tag.extend([tags[i]] * input_len) # all the sub-words from one word are labeled with the same tag\n",
        "\n",
        "        ids = ids[:config.MAX_LEN - 2] # leave space for [CLS][SEP]\n",
        "        target_tag = target_tag[:config.MAX_LEN - 2]\n",
        "\n",
        "        ids = [101] + ids + [102] # 101 -> CLS  102 -> SEP\n",
        "        target_tag = [0] + target_tag + [0]\n",
        "\n",
        "        mask = [1] * len(ids)\n",
        "        token_type_ids = [0] * len(ids)\n",
        "\n",
        "        padding_len = config.MAX_LEN - len(ids)\n",
        "\n",
        "        ids = ids + ([0] * padding_len) # make all the input the same length\n",
        "        mask = mask + ([0] * padding_len) # 0->acceptable 1->omitted\n",
        "        token_type_ids = token_type_ids + ([0] * padding_len)\n",
        "        target_tag = target_tag + ([0] * padding_len)\n",
        "\n",
        "        return {\n",
        "            \"ids\": torch.tensor(ids, dtype=torch.long),\n",
        "            \"mask\": torch.tensor(mask, dtype=torch.long),\n",
        "            \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n",
        "            \"target_tag\": torch.tensor(target_tag, dtype=torch.long),\n",
        "        }"
      ],
      "metadata": {
        "id": "zjTC8ew8aVJX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences, tag, enc_tag = process_data(config.TRAINING_FILE)\n",
        "\n",
        "meta_data = {\"enc_tag\": enc_tag}\n",
        "joblib.dump(meta_data, \"meta.bin\")\n",
        "\n",
        "num_tag = len(list(enc_tag.classes_))\n",
        "\n",
        "(train_sentences,teva_sentences,train_tag,teva_tag) = model_selection.train_test_split(sentences, tag, random_state=42, test_size=0.1)\n",
        "(test_sentences,valid_sentences,test_tag,valid_tag) = model_selection.train_test_split(teva_sentences, teva_tag, random_state=42, test_size=0.5)\n",
        "\n",
        "train_dataset = EntityDataset(texts=train_sentences, tags=train_tag)\n",
        "train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=config.TRAIN_BATCH_SIZE)\n",
        "\n",
        "valid_dataset = EntityDataset(texts=valid_sentences, tags=valid_tag)\n",
        "valid_data_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=config.TRAIN_BATCH_SIZE)\n",
        "\n",
        "test_dataset = EntityDataset(texts=test_sentences, tags=test_tag)\n",
        "test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=config.TRAIN_BATCH_SIZE)"
      ],
      "metadata": {
        "id": "XDHE43dlaivS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_fn(data_loader, model, optimizer, device, scheduler):\n",
        "    model.train()\n",
        "    final_loss = 0\n",
        "    i = 0 \n",
        "    for data in tqdm(data_loader, total=len(data_loader)):\n",
        "        for k, v in data.items():\n",
        "            data[k] = v.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        loss = model.Loss_fn(**data)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        final_loss += loss.item()\n",
        "        \n",
        "        i += 1\n",
        "        #if i % 100 == 0:\n",
        "        #  print(\"train_loss: {}\".format(loss.item()))\n",
        "\n",
        "    return final_loss / len(data_loader)\n",
        "\n",
        "\n",
        "def eval_fn(data_loader, model, device):\n",
        "    model.eval()\n",
        "    final_loss = 0\n",
        "\n",
        "    for data in tqdm(data_loader, total=len(data_loader)):\n",
        "        for k, v in data.items():\n",
        "            data[k] = v.to(device)\n",
        "        loss = model.Loss_fn(**data)\n",
        "        final_loss += loss.item()\n",
        "\n",
        "    return final_loss / len(data_loader)\n",
        "\n",
        "def loss_fn(output, target, mask, num_labels):\n",
        "    lfn = nn.CrossEntropyLoss()\n",
        "    active_loss = mask.view(-1) == 1\n",
        "    active_logits = output.view(-1, num_labels)\n",
        "    active_labels = torch.where(\n",
        "        active_loss,\n",
        "        target.view(-1),\n",
        "        torch.tensor(lfn.ignore_index).type_as(target)\n",
        "    )\n",
        "    loss = lfn(active_logits, active_labels)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "dsexWuapdU6O"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EntityModel(nn.Module):\n",
        "    def __init__(self, num_tag):\n",
        "        super(EntityModel, self).__init__()\n",
        "        self.num_tag = num_tag\n",
        "        self.bert = transformers.BertModel.from_pretrained(\n",
        "            config.BASE_MODEL_PATH\n",
        "        )\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.fc = nn.Linear(768, self.num_tag)\n",
        "        self.lstm = nn.LSTM(768, 768//2, num_layers=2, bidirectional=True, batch_first=True, dropout=0.3)\n",
        "        self.crf = CRF(self.num_tag, batch_first=True)\n",
        "    \n",
        "    def forward(self, ids, mask, token_type_ids, target_tag):\n",
        "        o1, _ = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids, return_dict=False)\n",
        "        seq_out, _ = self.lstm(o1)\n",
        "        tag = self.fc(seq_out)\n",
        "        crf_tag = self.crf.decode(tag, mask.bool())\n",
        "        return crf_tag\n",
        "\n",
        "    def Loss_fn(self, ids, mask, token_type_ids, target_tag):\n",
        "        o1, _ = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids, return_dict=False)\n",
        "        seq_out, _ = self.lstm(o1)\n",
        "        y_pred = self.fc(seq_out)\n",
        "        loss = -self.crf.forward(y_pred, target_tag, mask.bool(), reduction='mean')\n",
        "        return loss"
      ],
      "metadata": {
        "id": "8t2Fav5ZdYYv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\")\n",
        "model = EntityModel(num_tag=num_tag)\n",
        "model.to(device)\n",
        "\n",
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
        "optimizer_parameters = [\n",
        "    {\n",
        "        \"params\": [\n",
        "            p for n, p in param_optimizer if not any(\n",
        "                nd in n for nd in no_decay\n",
        "            )\n",
        "        ],\n",
        "        \"weight_decay\": 0.001,\n",
        "    },\n",
        "    {\n",
        "        \"params\": [\n",
        "            p for n, p in param_optimizer if any(\n",
        "                nd in n for nd in no_decay\n",
        "            )\n",
        "        ],\n",
        "        \"weight_decay\": 0.0,\n",
        "    },\n",
        "]\n",
        "\n",
        "num_train_steps = int(\n",
        "    len(train_sentences) / config.TRAIN_BATCH_SIZE * config.EPOCHS\n",
        ")\n",
        "optimizer = AdamW(optimizer_parameters, lr=4e-5)\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer, \n",
        "    num_warmup_steps=0, \n",
        "    num_training_steps=num_train_steps\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Ey8Fyjgddec",
        "outputId": "7a3d944f-614c-4663-8dda-f4dd36d9e193"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_loss = np.inf\n",
        "for epoch in range(config.EPOCHS):\n",
        "    train_loss = train_fn(\n",
        "        train_data_loader, \n",
        "        model, \n",
        "        optimizer, \n",
        "        device, \n",
        "        scheduler\n",
        "    )\n",
        "    test_loss = eval_fn(\n",
        "        valid_data_loader,\n",
        "        model,\n",
        "        device\n",
        "    )\n",
        "    print(f\"Train Loss = {train_loss} Valid Loss = {test_loss}\")\n",
        "    if test_loss < best_loss:\n",
        "        torch.save(model.state_dict(), config.MODEL_PATH)\n",
        "        best_loss = test_loss"
      ],
      "metadata": {
        "id": "ES8n-CA4d3Dg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N_dEwXUKEdn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\")\n",
        "model = EntityModel(num_tag=num_tag)\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/NLP/BertLstmCrf.bin'))\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "QIIYdz3xEdih"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "y_labels = []\n",
        "y_preds = []\n",
        "\n",
        "for data in tqdm(test_data_loader, total=len(test_data_loader)):\n",
        "    for k, v in data.items():\n",
        "        data[k] = v.to(device)\n",
        "    \n",
        "    for i in range(len(data[\"mask\"])):\n",
        "        mask = data[\"mask\"][i].cpu().numpy()\n",
        "        target = data[\"target_tag\"][i].cpu().numpy()\n",
        "        temp = []\n",
        "        for i in range(len(mask)):  \n",
        "          if mask[i] == 1:\n",
        "            temp.append(target[i])\n",
        "        y_preds.append(temp)\n",
        "        \n",
        "    \n",
        "    y_pred = model(**data)\n",
        "    for sentence in y_pred:\n",
        "      y_labels.append(sentence)\n",
        "\n",
        "y_true, y_pred = [], []\n",
        "for tags in y_labels:\n",
        "  for tag in tags:\n",
        "    y_true.append(tag)\n",
        "\n",
        "for tags in y_preds:\n",
        "  for tag in tags:\n",
        "    y_pred.append(tag)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hE5x4PfNKaDF",
        "outputId": "942a46e8-13ae-4b38-b9a7-e719842fe6a3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 38/38 [00:11<00:00,  3.32it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "target_names = list(enc_tag.classes_)\n",
        "print(classification_report(y_true, y_pred, target_names=target_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTg6jcstSSah",
        "outputId": "2e23e741-e9ac-4b09-cbce-c32f539d9c2c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-art       0.99      1.00      1.00      4816\n",
            "       B-eve       0.13      0.33      0.19         6\n",
            "       B-geo       0.91      0.84      0.87      3232\n",
            "       B-gpe       0.93      0.94      0.94       855\n",
            "       B-nat       0.08      0.33      0.12         3\n",
            "       B-org       0.68      0.80      0.74      1469\n",
            "       B-per       0.84      0.86      0.85      1305\n",
            "       B-tim       0.86      0.92      0.89      1070\n",
            "       I-art       0.21      0.40      0.28        10\n",
            "       I-eve       0.00      0.00      0.00         0\n",
            "       I-geo       0.82      0.68      0.74       545\n",
            "       I-gpe       0.25      1.00      0.40         3\n",
            "       I-nat       0.00      0.00      0.00         0\n",
            "       I-org       0.67      0.73      0.70      1000\n",
            "       I-per       0.93      0.87      0.90      1783\n",
            "       I-tim       0.79      0.85      0.82       311\n",
            "           O       0.99      0.99      0.99     47688\n",
            "\n",
            "    accuracy                           0.96     64096\n",
            "   macro avg       0.59      0.68      0.61     64096\n",
            "weighted avg       0.96      0.96      0.96     64096\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "meta_data = joblib.load(\"/content/drive/MyDrive/NLP/meta.bin\")\n",
        "enc_tag = meta_data[\"enc_tag\"]\n",
        "num_tag = len(list(enc_tag.classes_))\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "model = EntityModel(num_tag=17)\n",
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/NLP/BertLstmCrf.bin\"))\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "gN6ULAga8HKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(input_sentence, model):\n",
        "    meta_data = joblib.load(\"/content/drive/MyDrive/NLP/meta.bin\")\n",
        "    enc_tag = meta_data[\"enc_tag\"]\n",
        "\n",
        "    #input_sentence = \"Mr. Trump’s tweets began just moments after a Fox News report by Mike Tobin, a reporter for the network, about protests in Minnesota and elsewhere.\"\n",
        "    tokenized_sentence = config.TOKENIZER.encode(input_sentence)\n",
        "\n",
        "    input_sentence = input_sentence.split()\n",
        "\n",
        "    test_dataset = EntityDataset(texts=[input_sentence], tags=[[0]*len(input_sentence)])\n",
        "\n",
        "    with torch.no_grad():\n",
        "        data = test_dataset[0]\n",
        "        for k, v in data.items():\n",
        "            data[k] = v.to(device).unsqueeze(0)\n",
        "        y_pred = model(**data)[0]\n",
        "        \n",
        "        tokenized_sentence = config.TOKENIZER.convert_ids_to_tokens(tokenized_sentence)\n",
        "        tokenized_ner      = enc_tag.inverse_transform(y_pred)[:len(tokenized_sentence)]\n",
        "\n",
        "        new_ner = []\n",
        "        new_token = []\n",
        "\n",
        "        for i in range(1,len(tokenized_sentence)-1):\n",
        "          if tokenized_sentence[i].startswith(\"##\"):\n",
        "            new_token[-1] = new_token[-1] + tokenized_sentence[i][2:]\n",
        "          else:\n",
        "            new_token.append(tokenized_sentence[i])\n",
        "            new_ner.append(tokenized_ner[i])\n",
        "\n",
        "        for k,v in zip(new_token, new_ner):\n",
        "          print(v, k)"
      ],
      "metadata": {
        "id": "CiIpfBu_8HIR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"\"\"Mr. Trump’s tweets began just moments after a Fox \n",
        "            News report by Mike Tobin, a reporter for the network, \n",
        "            about protests in Minnesota and elsewhere.\"\"\"\n",
        "\n",
        "inference(sentence, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xu2QUDjt-m77",
        "outputId": "9540a190-08ce-4dd1-9435-2d2e6f149756"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "B-per mr\n",
            "B-per .\n",
            "I-per trump\n",
            "O ’\n",
            "O s\n",
            "O tweets\n",
            "O began\n",
            "O just\n",
            "O moments\n",
            "O after\n",
            "O a\n",
            "B-org fox\n",
            "I-org news\n",
            "O report\n",
            "O by\n",
            "B-per mike\n",
            "I-per tobin\n",
            "O ,\n",
            "O a\n",
            "O reporter\n",
            "O for\n",
            "O the\n",
            "O network\n",
            "O ,\n",
            "O about\n",
            "O protests\n",
            "O in\n",
            "B-geo minnesota\n",
            "O and\n",
            "O elsewhere\n",
            "O .\n"
          ]
        }
      ]
    }
  ]
}