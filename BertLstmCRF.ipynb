{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install pytorch-crf"
      ],
      "metadata": {
        "id": "wZQh9GRPWLsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import transformers\n",
        "from torchcrf import CRF\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn import model_selection\n",
        "\n",
        "from tqdm import tqdm\n",
        "from transformers import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup"
      ],
      "metadata": {
        "id": "ddNGR-U4ZWzx"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class config:\n",
        "    MAX_LEN = 128\n",
        "    TRAIN_BATCH_SIZE = 64\n",
        "    VALID_BATCH_SIZE = 64\n",
        "    EPOCHS = 20\n",
        "    BASE_MODEL_PATH = \"bert-base-uncased\"\n",
        "    MODEL_PATH = \"model.bin\"\n",
        "    TRAINING_FILE = \"/content/drive/MyDrive/NLP/ner_dataset.csv\"\n",
        "    TOKENIZER = transformers.BertTokenizer.from_pretrained(\n",
        "        BASE_MODEL_PATH,\n",
        "        do_lower_case=True\n",
        "    )\n",
        "\n",
        "def process_data(data_path):\n",
        "    df = pd.read_csv(data_path, encoding=\"latin-1\")\n",
        "    df.loc[:, \"Sentence #\"] = df[\"Sentence #\"].fillna(method=\"ffill\")\n",
        "\n",
        "    enc_tag = preprocessing.LabelEncoder()\n",
        "    \n",
        "    # fit_transform : a function that transforms the str labels into int labels\n",
        "    df.loc[:, \"Tag\"] = enc_tag.fit_transform(df[\"Tag\"]) # and return the int \n",
        "\n",
        "    sentences = df.groupby(\"Sentence #\")[\"Word\"].apply(list).values\n",
        "    tag = df.groupby(\"Sentence #\")[\"Tag\"].apply(list).values\n",
        "    return sentences, tag, enc_tag\n",
        "\n",
        "class EntityDataset:\n",
        "    def __init__(self, texts, tags):\n",
        "        self.texts = texts\n",
        "        self.tags = tags\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "    \n",
        "    def __getitem__(self, item):\n",
        "        text = self.texts[item]\n",
        "        tags = self.tags[item]\n",
        "\n",
        "        ids = []\n",
        "        target_tag =[]\n",
        "\n",
        "        for i, s in enumerate(text): \n",
        "            inputs = config.TOKENIZER.encode(\n",
        "                s,\n",
        "                add_special_tokens=False\n",
        "            )\n",
        "            # abhishek: ab ##hi ##sh ##ek\n",
        "            input_len = len(inputs)\n",
        "            ids.extend(inputs) # add all the sub-words\n",
        "            target_tag.extend([tags[i]] * input_len) # all the sub-words from one word are labeled with the same tag\n",
        "\n",
        "        ids = ids[:config.MAX_LEN - 2] # leave space for [CLS][SEP]\n",
        "        target_tag = target_tag[:config.MAX_LEN - 2]\n",
        "\n",
        "        ids = [101] + ids + [102] # 101 -> CLS  102 -> SEP\n",
        "        target_tag = [0] + target_tag + [0]\n",
        "\n",
        "        mask = [1] * len(ids)\n",
        "        token_type_ids = [0] * len(ids)\n",
        "\n",
        "        padding_len = config.MAX_LEN - len(ids)\n",
        "\n",
        "        ids = ids + ([0] * padding_len) # make all the input the same length\n",
        "        mask = mask + ([0] * padding_len) # 0->acceptable 1->omitted\n",
        "        token_type_ids = token_type_ids + ([0] * padding_len)\n",
        "        target_tag = target_tag + ([0] * padding_len)\n",
        "\n",
        "        return {\n",
        "            \"ids\": torch.tensor(ids, dtype=torch.long),\n",
        "            \"mask\": torch.tensor(mask, dtype=torch.long),\n",
        "            \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n",
        "            \"target_tag\": torch.tensor(target_tag, dtype=torch.long),\n",
        "        }"
      ],
      "metadata": {
        "id": "zjTC8ew8aVJX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences, tag, enc_tag = process_data(config.TRAINING_FILE)\n",
        "\n",
        "meta_data = {\"enc_tag\": enc_tag}\n",
        "joblib.dump(meta_data, \"meta.bin\")\n",
        "\n",
        "num_tag = len(list(enc_tag.classes_))\n",
        "\n",
        "(train_sentences,teva_sentences,train_tag,teva_tag) = model_selection.train_test_split(sentences, tag, random_state=42, test_size=0.1)\n",
        "(test_sentences,valid_sentences,test_tag,valid_tag) = model_selection.train_test_split(teva_sentences, teva_tag, random_state=42, test_size=0.5)\n",
        "\n",
        "train_dataset = EntityDataset(texts=train_sentences, tags=train_tag)\n",
        "train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=config.TRAIN_BATCH_SIZE)\n",
        "\n",
        "valid_dataset = EntityDataset(texts=valid_sentences, tags=valid_tag)\n",
        "valid_data_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=config.TRAIN_BATCH_SIZE)\n",
        "\n",
        "test_dataset = EntityDataset(texts=test_sentences, tags=test_tag)\n",
        "test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=config.TRAIN_BATCH_SIZE)"
      ],
      "metadata": {
        "id": "XDHE43dlaivS"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_fn(data_loader, model, optimizer, device, scheduler):\n",
        "    model.train()\n",
        "    final_loss = 0\n",
        "    i = 0 \n",
        "    for data in tqdm(data_loader, total=len(data_loader)):\n",
        "        for k, v in data.items():\n",
        "            data[k] = v.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        loss = model.Loss_fn(**data)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        final_loss += loss.item()\n",
        "        \n",
        "        i += 1\n",
        "        if i % 100 == 0:\n",
        "          print(\"train_loss: {}\".format(loss.item()))\n",
        "\n",
        "    return final_loss / len(data_loader)\n",
        "\n",
        "\n",
        "def eval_fn(data_loader, model, device):\n",
        "    model.eval()\n",
        "    final_loss = 0\n",
        "\n",
        "    for data in tqdm(data_loader, total=len(data_loader)):\n",
        "        for k, v in data.items():\n",
        "            data[k] = v.to(device)\n",
        "        loss = model.Loss_fn(**data)\n",
        "        final_loss += loss.item()\n",
        "\n",
        "    return final_loss / len(data_loader)\n",
        "\n",
        "def loss_fn(output, target, mask, num_labels):\n",
        "    lfn = nn.CrossEntropyLoss()\n",
        "    active_loss = mask.view(-1) == 1\n",
        "    active_logits = output.view(-1, num_labels)\n",
        "    active_labels = torch.where(\n",
        "        active_loss,\n",
        "        target.view(-1),\n",
        "        torch.tensor(lfn.ignore_index).type_as(target)\n",
        "    )\n",
        "    loss = lfn(active_logits, active_labels)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "dsexWuapdU6O"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EntityModel(nn.Module):\n",
        "    def __init__(self, num_tag):\n",
        "        super(EntityModel, self).__init__()\n",
        "        self.num_tag = num_tag\n",
        "        self.bert = transformers.BertModel.from_pretrained(\n",
        "            config.BASE_MODEL_PATH\n",
        "        )\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.fc = nn.Linear(768, self.num_tag)\n",
        "        self.lstm = nn.LSTM(768, 768//2, num_layers=2, bidirectional=True, batch_first=True, dropout=0.3)\n",
        "        self.crf = CRF(self.num_tag, batch_first=True)\n",
        "    \n",
        "    def forward(self, ids, mask, token_type_ids, target_tag):\n",
        "        o1, _ = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids, return_dict=False)\n",
        "        seq_out, _ = self.lstm(o1)\n",
        "        tag = self.fc(seq_out)\n",
        "        crf_tag = self.crf.decode(tag, mask.bool())\n",
        "        return crf_tag\n",
        "\n",
        "    def Loss_fn(self, ids, mask, token_type_ids, target_tag):\n",
        "        o1, _ = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids, return_dict=False)\n",
        "        seq_out, _ = self.lstm(o1)\n",
        "        y_pred = self.fc(seq_out)\n",
        "        loss = -self.crf.forward(y_pred, target_tag, mask.bool(), reduction='mean')\n",
        "        return loss"
      ],
      "metadata": {
        "id": "8t2Fav5ZdYYv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\")\n",
        "model = EntityModel(num_tag=num_tag)\n",
        "model.to(device)\n",
        "\n",
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
        "optimizer_parameters = [\n",
        "    {\n",
        "        \"params\": [\n",
        "            p for n, p in param_optimizer if not any(\n",
        "                nd in n for nd in no_decay\n",
        "            )\n",
        "        ],\n",
        "        \"weight_decay\": 0.001,\n",
        "    },\n",
        "    {\n",
        "        \"params\": [\n",
        "            p for n, p in param_optimizer if any(\n",
        "                nd in n for nd in no_decay\n",
        "            )\n",
        "        ],\n",
        "        \"weight_decay\": 0.0,\n",
        "    },\n",
        "]\n",
        "\n",
        "num_train_steps = int(\n",
        "    len(train_sentences) / config.TRAIN_BATCH_SIZE * config.EPOCHS\n",
        ")\n",
        "optimizer = AdamW(optimizer_parameters, lr=3e-5)\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer, \n",
        "    num_warmup_steps=0, \n",
        "    num_training_steps=num_train_steps\n",
        ")"
      ],
      "metadata": {
        "id": "4Ey8Fyjgddec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_loss = np.inf\n",
        "for epoch in range(config.EPOCHS):\n",
        "    train_loss = train_fn(\n",
        "        train_data_loader, \n",
        "        model, \n",
        "        optimizer, \n",
        "        device, \n",
        "        scheduler\n",
        "    )\n",
        "    test_loss = eval_fn(\n",
        "        valid_data_loader,\n",
        "        model,\n",
        "        device\n",
        "    )\n",
        "    print(f\"Train Loss = {train_loss} Valid Loss = {test_loss}\")\n",
        "    if test_loss < best_loss:\n",
        "        torch.save(model.state_dict(), config.MODEL_PATH)\n",
        "        best_loss = test_loss"
      ],
      "metadata": {
        "id": "ES8n-CA4d3Dg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N_dEwXUKEdn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\")\n",
        "model = EntityModel(num_tag=num_tag)\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/NLP/BertLstmCrf.bin'))\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "QIIYdz3xEdih"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "y_labels = []\n",
        "y_preds = []\n",
        "\n",
        "for data in tqdm(test_data_loader, total=len(test_data_loader)):\n",
        "    for k, v in data.items():\n",
        "        data[k] = v.to(device)\n",
        "    \n",
        "    for i in range(len(data[\"mask\"])):\n",
        "        mask = data[\"mask\"][i].cpu().numpy()\n",
        "        target = data[\"target_tag\"][i].cpu().numpy()\n",
        "        temp = []\n",
        "        for i in range(len(mask)):  \n",
        "          if mask[i] == 1:\n",
        "            temp.append(target[i])\n",
        "        y_preds.append(temp)\n",
        "        \n",
        "    \n",
        "    y_pred = model(**data)\n",
        "    for sentence in y_pred:\n",
        "      y_labels.append(sentence)\n",
        "\n",
        "y_true, y_pred = [], []\n",
        "for tags in y_labels:\n",
        "  for tag in tags:\n",
        "    y_true.append(tag)\n",
        "\n",
        "for tags in y_preds:\n",
        "  for tag in tags:\n",
        "    y_pred.append(tag)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hE5x4PfNKaDF",
        "outputId": "7512a31f-746b-4840-ed14-b6611bc5a2dc"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 38/38 [00:23<00:00,  1.63it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "target_names = list(enc_tag.classes_)\n",
        "print(classification_report(y_true, y_pred, target_names=target_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTg6jcstSSah",
        "outputId": "04078530-316e-4103-ed92-fdff568b5d5e"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-art       1.00      1.00      1.00      4817\n",
            "       B-eve       0.00      0.00      0.00         0\n",
            "       B-geo       0.89      0.85      0.87      3135\n",
            "       B-gpe       0.92      0.93      0.92       859\n",
            "       B-nat       0.00      0.00      0.00         0\n",
            "       B-org       0.75      0.76      0.76      1706\n",
            "       B-per       0.81      0.89      0.85      1213\n",
            "       B-tim       0.86      0.92      0.89      1081\n",
            "       I-art       0.00      0.00      0.00         0\n",
            "       I-eve       0.00      0.00      0.00         0\n",
            "       I-geo       0.71      0.80      0.75       405\n",
            "       I-gpe       0.00      0.00      0.00         0\n",
            "       I-nat       0.00      0.00      0.00         0\n",
            "       I-org       0.75      0.68      0.71      1204\n",
            "       I-per       0.92      0.88      0.90      1728\n",
            "       I-tim       0.82      0.79      0.80       347\n",
            "           O       0.99      0.99      0.99     47601\n",
            "\n",
            "    accuracy                           0.96     64096\n",
            "   macro avg       0.55      0.56      0.56     64096\n",
            "weighted avg       0.96      0.96      0.96     64096\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    }
  ]
}